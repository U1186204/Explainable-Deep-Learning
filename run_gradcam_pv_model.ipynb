{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1211a001",
   "metadata": {},
   "source": [
    "## Explainable Deep Learning\n",
    "\n",
    "**Name: Ilseop(Shawn) Lee**\n",
    "\n",
    "### Instructions\n",
    "In this assignment, you will work with pretrained deep learning models to investigate model explainability in computer vision. Your objective is to apply GradCAM and at least two of its variants to a meaningful image classification problem of your choice and analyze *how* and *why* the model makes its decisions.\n",
    "\n",
    "You are encouraged to select an image classification task that holds personal or societal significance. Potential areas include, but are not limited to: wildlife conservation, road safety, public health, environmental sustainability, or social impact. You may use an existing public dataset or a curated subset, and pretrained models such as ResNet-50 or Vision Transformers (ViT).\n",
    "\n",
    "\n",
    "**Tasks**\n",
    "Choose an image classification problem relevant to you (e.g., wildlife detection, object recognition in autonomous driving, or recycling classification).<br>\n",
    "Use a pretrained computer vision model (e.g., ResNet-50, ViT) for your classification task. Transfer learning is optional.<br>\n",
    "Apply Explainability Techniques:<br>\n",
    "- Implement GradCAM and at least two GradCAM variants\n",
    "- Apply these techniques to at least 5 images from your dataset\n",
    "- Generate and present visualizations showing what regions of the image the model is focusing on for its predictions.\n",
    "- Compare and contrast the attention maps generated by GradCAM and its variants.\n",
    "\n",
    "Reflection:\n",
    "- Discuss the visual cues the model attends to\n",
    "- Comment on any surprising or misleading behavior\n",
    "- Reflect on why model explainability is important in your selected application domain\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab0738",
   "metadata": {},
   "source": [
    "- **Rooftop Solar Energy System Detection in Cape Town**\n",
    "    - Task : Detect and classify Solar Energy Systems (Solar Panel, Water Heater, Pool Heater) from aerial imagery\n",
    "    - Purpose : Assess spatial adoption patterns of solar energy system and provide evicende to suppoert energy policy and planning in Cape Town\n",
    "\n",
    "*Note: This was my personal project('Energy Transition During Energy Crisis: Cape Town's Experience'). Therefore, I used the dataset and model that I had processed for that project*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393361c",
   "metadata": {},
   "source": [
    "**Model and Data Preparation**\n",
    "- Pre-trained Segmentation Mdoel\n",
    "    - ResNext-50 encoder with Feature Pyramid Network Decoder\n",
    "    - Loaded from a best performing checkpoint in ckpt format\n",
    "\n",
    "- Dataset\n",
    "    - Aerial Imagery from Cape Town (Cropped to 320*320)\n",
    "\n",
    "- Classification Task\n",
    "    - Class 0: Background, Class 1: Solar_Panel, Class 2: Water_heater, Class 3: Pool_heater\n",
    "\n",
    "- Visualization\n",
    "    - For Clarity, Solar Panels = 'Green', Water heater = 'Red', and Pool heater = 'Blue'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb086f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and dataset are ready.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, torch, numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset as BaseDataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Class mapping\n",
    "CLASSES = [\"background\", \"Solar_Panel\", \"Water_heater\", \"Pool_heater\"]\n",
    "COLORS = {0:(0,0,0), 1:(0,255,0), 2:(255,0,0), 3:(0,0,255)}\n",
    "\n",
    "# Paths\n",
    "CKPT_PATH = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/pv-model-epoch=38-valid_avg_PV_iou=0.9572.ckpt\"\n",
    "IMG_DIR   = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/Test_Images_Prd\"\n",
    "OUT_BASE  = \"/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts\"\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "class PVModel(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1,3,1,1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1,3,1,1))\n",
    "    def forward(self, image):\n",
    "        image = (image - self.mean) / self.std\n",
    "        return self.model(image)\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "def get_validation_augmentation():\n",
    "    return A.Compose([\n",
    "        A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n",
    "        A.CenterCrop(height=320, width=320),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class InferenceDataset(BaseDataset):\n",
    "    def __init__(self, image_dir, augmentation=None):\n",
    "        self.image_paths = [os.path.join(image_dir,f) for f in os.listdir(image_dir)]\n",
    "        self.augmentation = augmentation\n",
    "    def __len__(self): return len(self.image_paths)\n",
    "    def __getitem__(self,i):\n",
    "        img_path = self.image_paths[i]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape[:2] != (320,320):\n",
    "            image = cv2.resize(image,(320,320))\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image = sample[\"image\"]\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2,0,1)).float()\n",
    "        return image, str(img_path)\n",
    "\n",
    "# ---------------- Model loading ----------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PVModel.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    arch=\"FPN\",\n",
    "    encoder_name=\"resnext50_32x4d\",\n",
    "    in_channels=3,\n",
    "    out_classes=len(CLASSES),\n",
    "    strict=False\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset = InferenceDataset(IMG_DIR, augmentation=get_validation_augmentation())\n",
    "loader  = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "target_layer = model.model.encoder.layer4[-1].conv3\n",
    "print(\"Model and dataset are ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112006a3",
   "metadata": {},
   "source": [
    "**Grad-CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de70bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_results\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Grad-CAM ----------------\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients, self.activations = None, None\n",
    "        self.fwd_hook = target_layer.register_forward_hook(self.save_activation)\n",
    "        self.bwd_hook = target_layer.register_full_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, input_tensor, target_class):\n",
    "        logits = self.model(input_tensor)\n",
    "        target = logits[:, target_class, :, :].mean()\n",
    "        self.model.zero_grad()\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        # Compute class activation map\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam_map = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam_map = torch.relu(cam_map).squeeze().cpu().numpy()\n",
    "        cam_map = (cam_map - cam_map.min()) / (cam_map.max() + 1e-8)\n",
    "        return cam_map, logits\n",
    "\n",
    "def overlay_heatmap(img, cam_map, alpha=0.5):\n",
    "    cam_map_resized = cv2.resize(cam_map, (img.shape[1], img.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_map_resized), cv2.COLORMAP_JET)\n",
    "    return cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
    "\n",
    "# Output directory\n",
    "outdir_gc = os.path.join(OUT_BASE, \"gradcam_results\")\n",
    "os.makedirs(outdir_gc, exist_ok=True)\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "# Generate Grad-CAM visualizations\n",
    "for imgs, img_paths in loader:\n",
    "    imgs = imgs.float().to(device)\n",
    "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
    "    orig = cv2.imread(img_path)\n",
    "\n",
    "    for cls_idx in [1, 2, 3]:\n",
    "        cam_map, _ = gradcam(imgs, cls_idx)\n",
    "        overlay = overlay_heatmap(orig, cam_map)\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        cv2.imwrite(os.path.join(outdir_gc, f\"{base}_gradcam_class{cls_idx}.png\"), overlay)\n",
    "\n",
    "print(f\"Grad-CAM visualizations saved to: {outdir_gc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc4b20",
   "metadata": {},
   "source": [
    "**Grad-CAM++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87b2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM++ visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_results_plusplus\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Grad-CAM++ ----------------\n",
    "class GradCAMPlusPlus(GradCAM):\n",
    "    def __call__(self, input_tensor, target_class):\n",
    "        logits = self.model(input_tensor)\n",
    "        target = logits[:, target_class, :, :].mean()\n",
    "        self.model.zero_grad()\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        grads, acts = self.gradients, self.activations\n",
    "        grads2, grads3 = grads ** 2, grads ** 3\n",
    "\n",
    "        numerator = grads2\n",
    "        denominator = 2 * grads2 + (acts * grads3).sum(dim=(2, 3), keepdim=True) + 1e-8\n",
    "        alphas = numerator / denominator\n",
    "        weights = (alphas * torch.relu(grads)).sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "        # Compute Grad-CAM++ map\n",
    "        cam_map = (weights * acts).sum(dim=1, keepdim=True)\n",
    "        cam_map = torch.relu(cam_map).squeeze().cpu().numpy()\n",
    "        cam_map = (cam_map - cam_map.min()) / (cam_map.max() + 1e-8)\n",
    "        return cam_map, logits\n",
    "\n",
    "# Output directory\n",
    "outdir_pp = os.path.join(OUT_BASE, \"gradcam_results_plusplus\")\n",
    "os.makedirs(outdir_pp, exist_ok=True)\n",
    "\n",
    "gradcam_pp = GradCAMPlusPlus(model, target_layer)\n",
    "\n",
    "# Generate Grad-CAM++ visualizations\n",
    "for imgs, img_paths in loader:\n",
    "    imgs = imgs.float().to(device)\n",
    "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
    "    orig = cv2.imread(img_path)\n",
    "\n",
    "    for cls_idx in [1, 2, 3]:\n",
    "        cam_map, _ = gradcam_pp(imgs, cls_idx)\n",
    "        overlay = overlay_heatmap(orig, cam_map)\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        cv2.imwrite(os.path.join(outdir_pp, f\"{base}_gradcampp_class{cls_idx}.png\"), overlay)\n",
    "\n",
    "print(f\"Grad-CAM++ visualizations saved to: {outdir_pp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2522508",
   "metadata": {},
   "source": [
    "**Guided_Grad_CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided Grad-CAM visualizations saved to: /Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/gradcam_guided\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Guided Grad-CAM ----------------\n",
    "import copy\n",
    "\n",
    "# 1) Define Guided Backpropagation ReLU\n",
    "class GuidedBackpropReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.relu(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (input,) = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        grad_input[grad_output < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "# 2) Replace all ReLUs in the model with GuidedBackpropReLU\n",
    "def replace_relu_with_guided(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            module._modules[name] = nn.ReLU(inplace=False)\n",
    "            module._modules[name].forward = lambda x: GuidedBackpropReLU.apply(x)\n",
    "        else:\n",
    "            replace_relu_with_guided(child)\n",
    "\n",
    "# 3) Create a copy of the model with guided ReLUs\n",
    "guided_model = copy.deepcopy(model)\n",
    "replace_relu_with_guided(guided_model)\n",
    "guided_model.eval()\n",
    "\n",
    "# 4) Use the same Grad-CAM instance\n",
    "gradcam_guided = GradCAM(model, target_layer)\n",
    "\n",
    "# 5) Output directory\n",
    "outdir_guided = os.path.join(OUT_BASE, \"gradcam_guided\")\n",
    "os.makedirs(outdir_guided, exist_ok=True)\n",
    "\n",
    "# 6) Run Guided Grad-CAM\n",
    "for imgs, img_paths in loader:\n",
    "    imgs = imgs.float().to(device)\n",
    "    img_path = img_paths if isinstance(img_paths, str) else img_paths[0]\n",
    "    orig = cv2.imread(img_path)\n",
    "\n",
    "    for cls_idx in [1, 2, 3]:\n",
    "        # Compute CAM\n",
    "        cam_map, _ = gradcam_guided(imgs, cls_idx)\n",
    "\n",
    "        # Guided Backpropagation\n",
    "        imgs.requires_grad = True\n",
    "        logits = guided_model(imgs)\n",
    "        target = logits[:, cls_idx, :, :].mean()\n",
    "        guided_model.zero_grad()\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        guided_grad = imgs.grad.detach().cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        guided_grad = (guided_grad - guided_grad.min()) / (guided_grad.max() + 1e-8)\n",
    "\n",
    "        # Guided Grad-CAM = Guided Backprop * CAM\n",
    "        cam_resized = cv2.resize(cam_map, (guided_grad.shape[1], guided_grad.shape[0]))\n",
    "        guided_cam = guided_grad * cam_resized[..., np.newaxis]\n",
    "        guided_cam = (guided_cam - guided_cam.min()) / (guided_cam.max() + 1e-8)\n",
    "        guided_cam = np.uint8(255 * guided_cam)\n",
    "\n",
    "        # Save visualization\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        cv2.imwrite(os.path.join(outdir_guided, f\"{base}_guidedcam_class{cls_idx}.png\"), guided_cam)\n",
    "\n",
    "print(f\"Guided Grad-CAM visualizations saved to: {outdir_guided}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4fe77",
   "metadata": {},
   "source": [
    "![sanple1](/Users/ilseoplee/XAI_AIPI590.01_2025Fall/Week6_artifacts/picture22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f027a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_11env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
